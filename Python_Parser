#!/usr/bin/env python
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException,ElementNotVisibleException,TimeoutException
import re, time, urllib.parse, urllib.request, urllib.error, csv

""" All input - our vacancy query and place to save the file with results """
# Parsing
query='xxxx' # what we are looking for
path=r'xxx' #file folder
scope=50 # How many pages we analyze
path_to_chromedriver=r'C:\chromedriver.exe'
# Auto-application
resume_name='xxxxx'
cover_letter='Hello! I like this vacancy!'

my_username='xxxx@gmail.com'
my_password='xxx'

""" ------------------------------"""
base_url_0='https://www.hh.ru/account/login'
base_url = ('http://hh.ru/search/vacancy?text={}&'
       'only_with_salary=false&area=1&enable_snippets=true&'
       'clusters=true&salary='.format(urllib.parse.quote(query))) # cyrillic decoded by urllin.parse.quote() OR it is possible to use 'requests' library #First page


base_url2= ('http://hh.ru/search/vacancy?'
            'text={}&enable_snippets=true&clusters=true&area=1&'.format(format(urllib.parse.quote(query))))  #all other pages incremented by page=1,2....

apply_url='http://hh.ru/applicant/vacancy_response?vacancyId='



def get_html(url):  # read url
    global response
    try:
        response=urllib.request.urlopen(url)
    except urllib.error.HTTPError:
        pass
    return response.read()

def parse(html): # analyze HTML
    soup=BeautifulSoup(html, "html.parser")
    table=soup.find('td', class_='l-cell l-cell_search') # Table with a table where vacancies are placed
    vacancy_table=table.find_all('div', class_='search-result-item_standard') #select rows with single vacancy
    vacancies=[]
    for row in vacancy_table[0:]:
        vacancies.append({
        'position': [position.text for position in row.find_all('a', class_='search-result-item__name search-result-item__name_standard')],
        'company': [company.text for company in row.find_all('a', class_='link-secondary')], 
        'salary': [salary.text for salary in row.find_all('div', class_='b-vacancy-list-salary')], 
        'description': [description.text for description in row.find_all('div', class_='search-result-item__snippet')], 
        'publication_date': [pub.text for pub in row.find_all('span', class_="b-vacancy-list-date")],
        'link': [re.search(r'.+?(?=\?)',link['href']).group()  for link in row.find_all('a', class_="search-result-item__name search-result-item__name_standard",href=True)]
        })
        
    print('Все вакансии обработаны')
        
    return vacancies    

def csv_file(vacancies):  # We save all vacancies in .csv file to analyze in Excel afterwards
    with open(path,'w',encoding='cp1251') as csvfile:
        writer=csv.writer(csvfile,delimiter=';',lineterminator='\n')
        writer.writerow(('Компания','Зарплата','Дата вакансии','Имя позиции','Описание','Ссылка'))
        for vacancy in vacancies:
            writer.writerow((vacancy['company'],', '.join(vacancy['salary']),', '.join(vacancy['publication_date']),', '.join(vacancy['position']),', '.join(vacancy['description']),', '.join(vacancy['link'])))
    print('Файл с вакансиями сохранен')
    csvfile.close()
    

        
def login_apply(url, vacancies):
    
    driver = webdriver.Chrome(path_to_chromedriver)
    driver.get(base_url_0)
    
    
    username = driver.find_element_by_name('username')
    password = driver.find_element_by_name('password')
    username.send_keys(my_username)
    password.send_keys(my_password)
    driver.find_element_by_xpath("//input[@type='submit']").click()
    
    for vacancy in vacancies:
        
        driver.get(apply_url+re.search(r'[0-9]+',vacancy['link'][0]).group())
        no_mistake=True
        while no_mistake:
            try:
                try:
                    letter=driver.find_element_by_name('letter')
                    letter.send_keys(cover_letter) 
                except (ElementNotVisibleException,TimeoutException) as e:
                    pass
                try:
                    driver.find_element_by_xpath("//input[@type='submit']").click()
                    no_mistake=True
                except (ElementNotVisibleException,TimeoutException) as e:
                    no_mistake=False
                    time.sleep(2)
                
            except (NoSuchElementException, TimeoutException) as e:  
                no_mistake=False
                time.sleep(2)
                
    driver.close()
    print('БОТ завершил свою работу')


def main():
    
    vacancies=[]
    
    for i in range(1,scope):
        print('Обработано %d страниц выдачи' % (i) )
        if i==1:
            vacancies.extend(parse(get_html(base_url)))
        else:
            vacancies.extend(parse(get_html(base_url2+'page=%s' % i)))
            
    csv_file(vacancies)   
    
    login_apply(base_url_0,vacancies)
    

if __name__=='__main__':
    main()
