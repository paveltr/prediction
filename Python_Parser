#!/usr/bin/env python
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
import csv
import urllib.parse
import urllib.request
import urllib.error


""" All input - our vacancy query and place to save the file with results """
query='Инженер' # what we are looking for
path=r'C:\Users\Пашка\Documents\HH_parsed.csv' #file folder
scope=150 # How many pages we analyze
""" ------------------------------"""



query = 'инженер'
 
base_url = ('http://hh.ru/search/vacancy?text={}&'
       'only_with_salary=false&area=1&enable_snippets=true&'
       'clusters=true&salary='.format(urllib.parse.quote(query))) # cyrillic decoded by urllin.parse.quote() OR it is possible to use 'requests' library #First page


base_url2= ('http://hh.ru/search/vacancy?'
            'text={}&enable_snippets=true&clusters=true&area=1&'.format(format(urllib.parse.quote(query))))  #all other pages incremented by page=1,2....


def get_html(url):  # read url
    global response
    try:
        response=urllib.request.urlopen(url)
    except urllib.error.HTTPError:
        print('FINISHED (forget error message)')
    return response.read()

def parse(html): # analyze HTML
    soup=BeautifulSoup(html, "html.parser")
    table=soup.find('td', class_='l-cell l-cell_search') # Table with a table where vacancies are placed
    vacancy_table=table.find_all('div', class_='search-result-item_standard') #select rows with single vacancy
    vacancies=[]
    for row in vacancy_table[0:]:
        vacancies.append({
        'position': [position.text for position in row.find_all('a', class_='search-result-item__name search-result-item__name_standard')],
        'company': [company.text for company in row.find_all('a', class_='link-secondary')], 
        'salary': [salary.text for salary in row.find_all('div', class_='b-vacancy-list-salary')], 
        'description': [description.text for description in row.find_all('div', class_='search-result-item__snippet')], 
        'publication_date': [pub.text for pub in row.find_all('span', class_="b-vacancy-list-date")],
        'link': [link['href'] for link in row.find_all('a', class_="search-result-item__name search-result-item__name_standard",href=True)]
        })
        
    return vacancies    

def csv_file(vacancies):  # We save all vacancies in .csv file to analyze in Excel afterwards
    with open(path,'w') as csvfile:
        writer=csv.writer(csvfile,delimiter=';',lineterminator='\n')
        writer.writerow(('Компания','Зарплата','Дата вакансии','Имя позиции','Описание','Ссылка'))
        for vacancy in vacancies:
            writer.writerow((vacancy['company'],', '.join(vacancy['salary']),', '.join(vacancy['publication_date']),', '.join(vacancy['position']),', '.join(vacancy['description']),', '.join(vacancy['link'])))
        


def main():
    
    #print(parse(get_html('http://hh.ru/search/vacancy?clusters=true&area=1&enable_snippets=true&text=%D0%B8%D0%BD%D0%B6%D0%B5%D0%BD%D0%B5%D1%80&page=100')))
    vacancies=[]
    
    for i in range(1,scope):
        print('Обработано %d страниц' % (i),'(%.1f%%)' % (i*100/scope))
        if i==1:
            vacancies.extend(parse(get_html(base_url)))
        else:
            vacancies.extend(parse(get_html(base_url2+'page=%s' % i)))
    print(vacancies)
    csv_file(vacancies)   

if __name__=='__main__':
    main()
